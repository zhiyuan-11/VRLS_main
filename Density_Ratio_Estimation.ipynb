{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPCSfwRg54E78LTKttYK0yw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ===============================================\n","# Block [1]: Install necessary libraries (Colab)\n","# ===============================================\n","# If already installed, comment these out\n","!pip install cvxpy tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zvFJvuPqCeNg","executionInfo":{"status":"ok","timestamp":1742551348420,"user_tz":-60,"elapsed":5986,"user":{"displayName":"zhiyuan wu","userId":"11247997516539609334"}},"outputId":"7609420c-6525-46c9-c2a1-5e57dc9cc6ab"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: cvxpy in /usr/local/lib/python3.11/dist-packages (1.6.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (0.6.7.post3)\n","Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (0.10.0)\n","Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (3.2.7.post2)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (2.0.2)\n","Requirement already satisfied: scipy>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (1.14.1)\n","Requirement already satisfied: qdldl in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy) (0.1.7.post5)\n"]}]},{"cell_type":"code","source":["# ===============================================\n","# Block [2]: Import libraries\n","# ===============================================\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torchvision import models, transforms, datasets\n","from torch.utils.data import DataLoader, Subset\n","\n","import cvxpy as cp\n","import numpy as np\n","import random\n","import os\n","import requests\n","import zipfile\n","from tqdm import tqdm\n","from collections import defaultdict, Counter\n","\n","# Check device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooZH83O9Ce67","executionInfo":{"status":"ok","timestamp":1742551355184,"user_tz":-60,"elapsed":6762,"user":{"displayName":"zhiyuan wu","userId":"11247997516539609334"}},"outputId":"5e27972e-3c82-4518-da1e-72f929d12d62"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["# ===============================================\n","# Block [3]: Build the ResNet18 model for 200 classes\n","# ===============================================\n","def create_resnet18_for_200_classes():\n","    \"\"\"\n","    Create a ResNet18 model with pretrained weights on ImageNet.\n","    Then replace the final fully connected layer to output 200 classes.\n","    Freeze all layers except layer4 and fc, so only these two will be trained.\n","    \"\"\"\n","    # Load pretrained ResNet18\n","    model = models.resnet18(pretrained=True)\n","\n","    # Adjust the final layer to output 200 classes\n","    num_features = model.fc.in_features\n","    model.fc = nn.Linear(num_features, 200)\n","\n","    # Freeze everything except layer4 and fc\n","    for name, param in model.named_parameters():\n","        if not (name.startswith(\"layer4\") or name.startswith(\"fc\")):\n","            param.requires_grad = False\n","\n","    return model"],"metadata":{"id":"qxqRv-wzCgLz","executionInfo":{"status":"ok","timestamp":1742551355186,"user_tz":-60,"elapsed":1,"user":{"displayName":"zhiyuan wu","userId":"11247997516539609334"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ===============================================\n","# Block [4]: Training and validation functions\n","# ===============================================\n","def train_model(\n","    model,\n","    train_loader,\n","    val_loader,\n","    criterion,\n","    optimizer,\n","    num_epochs=5,\n","    use_entropy_regularizer=False,\n","    entropy_weight=0.1\n","):\n","    \"\"\"\n","    Train the model on train_loader and evaluate on val_loader.\n","    Optionally, apply Shannon entropy regularization to maximize\n","    the entropy of softmax outputs.\n","    \"\"\"\n","    model.to(device)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        print(f\"\\nEpoch {epoch+1}/{num_epochs} ----------------------------\")\n","        for inputs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","\n","            # Base cross entropy\n","            ce_loss = criterion(outputs, labels)\n","\n","            # Optional: add negative Shannon entropy for regularization\n","            if use_entropy_regularizer:\n","                softmax_outputs = F.softmax(outputs, dim=1)\n","                # Shannon entropy = -sum(p * log(p))\n","                # We want to maximize it => add negative to the loss\n","                shannon_entropy = -torch.sum(\n","                    softmax_outputs * torch.log(softmax_outputs + 1e-8),\n","                    dim=1\n","                )\n","                entropy_loss = -entropy_weight * shannon_entropy.mean()\n","                loss = ce_loss + entropy_loss\n","            else:\n","                loss = ce_loss\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","        train_loss = running_loss / total\n","        train_acc = correct / total\n","\n","        # Validation phase\n","        val_loss, val_acc = validate_model(model, val_loader, criterion)\n","\n","        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n","              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","def validate_model(model, val_loader, criterion):\n","    \"\"\"\n","    Evaluate the model on the validation set.\n","    Returns (val_loss, val_accuracy).\n","    \"\"\"\n","    model.eval()\n","    val_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            val_loss += loss.item() * inputs.size(0)\n","            _, predicted = outputs.max(1)\n","            val_total += labels.size(0)\n","            val_correct += predicted.eq(labels).sum().item()\n","\n","    val_loss /= val_total\n","    val_acc = val_correct / val_total\n","    return val_loss, val_acc"],"metadata":{"id":"J_MLELZvCioW","executionInfo":{"status":"ok","timestamp":1742551355188,"user_tz":-60,"elapsed":1,"user":{"displayName":"zhiyuan wu","userId":"11247997516539609334"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# ===============================================\n","# Block [5]: ModelWithTemperature for temperature scaling\n","# ===============================================\n","class ModelWithTemperature(nn.Module):\n","    \"\"\"\n","    A wrapper that applies temperature scaling to a given model's logits.\n","    Temperature is optimized using the validation set to minimize NLL.\n","    \"\"\"\n","    def __init__(self, model):\n","        super(ModelWithTemperature, self).__init__()\n","        self.model = model\n","        self.temperature = nn.Parameter(torch.ones(1) * 1.0)\n","\n","    def forward(self, x):\n","        logits = self.model(x)\n","        return logits / self.temperature\n","\n","    def set_temperature(self, valid_loader):\n","        \"\"\"\n","        Optimize temperature on the validation set using LBFGS to minimize NLL.\n","        \"\"\"\n","        self.model.eval()\n","        nll_criterion = nn.CrossEntropyLoss()\n","\n","        # Collect logits and labels from validation set\n","        logits_list, labels_list = [], []\n","        with torch.no_grad():\n","            for inputs, labels in valid_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                logits = self.model(inputs)\n","                logits_list.append(logits)\n","                labels_list.append(labels)\n","\n","        logits = torch.cat(logits_list).to(device)\n","        labels = torch.cat(labels_list).to(device)\n","\n","        # Optimize temperature with LBFGS\n","        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n","\n","        def eval_step():\n","            optimizer.zero_grad()\n","            loss = nll_criterion(logits / self.temperature, labels)\n","            loss.backward()\n","            return loss\n","\n","        optimizer.step(eval_step)\n","        print(f\"Optimal Temperature: {self.temperature.item():.3f}\")\n","\n","        return self"],"metadata":{"id":"WVxNPXU4Ckff","executionInfo":{"status":"ok","timestamp":1742551355189,"user_tz":-60,"elapsed":1,"user":{"displayName":"zhiyuan wu","userId":"11247997516539609334"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ===============================================\n","# Block [6]: Convex optimization & MSE calculation\n","# ===============================================\n","def get_test_probabilities(model_with_temp, test_loader):\n","    \"\"\"\n","    Obtain softmax probabilities from the (temperature-scaled) model on the test set.\n","    \"\"\"\n","    model_with_temp.eval()\n","    all_probs = []\n","\n","    with torch.no_grad():\n","        for inputs, _ in test_loader:\n","            inputs = inputs.to(device)\n","            logits = model_with_temp(inputs)\n","            probs = F.softmax(logits, dim=1)\n","            all_probs.append(probs.cpu())\n","\n","    return torch.cat(all_probs, dim=0)\n","\n","def estimate_r_with_cvxpy(f_x, num_classes=200):\n","    \"\"\"\n","    Use cvxpy to estimate r that maximizes the sum of log(f_x @ r).\n","    Constraints: r >= 0, sum(r) = 1.\n","    \"\"\"\n","    f_x_np = f_x.numpy()\n","    r = cp.Variable(num_classes, nonneg=True)\n","    log_likelihood = cp.sum(cp.log(f_x_np @ r + 1e-8))\n","    constraints = [cp.sum(r) == 1]\n","    problem = cp.Problem(cp.Maximize(log_likelihood), constraints)\n","    problem.solve(solver=cp.SCS, max_iters=500)\n","    return torch.tensor(r.value, device=device)\n","\n","def calculate_mse(predicted, actual):\n","    \"\"\"\n","    Calculate the RMSE between two vectors.\n","    \"\"\"\n","    return torch.sqrt(torch.mean((predicted - actual) ** 2)).item()\n","\n","def evaluate_on_test_subsets(model_with_temp, test_loaders, test_distributions, num_classes=200):\n","    \"\"\"\n","    For each test subset, compute predicted distribution r via convex optimization,\n","    compare with true distribution to get MSE, and finally compute the mean/std of MSE.\n","    \"\"\"\n","    all_mse = []\n","\n","    for i, (test_loader, true_dist) in enumerate(zip(test_loaders, test_distributions)):\n","        print(f\"\\nEvaluating on Test Subset {i+1}...\")\n","        # 1) get probabilities from model\n","        f_x = get_test_probabilities(model_with_temp, test_loader)\n","        # 2) estimate r using cvxpy\n","        estimated_r = estimate_r_with_cvxpy(f_x, num_classes=num_classes)\n","        # 3) compute MSE against true_dist\n","        if isinstance(true_dist, np.ndarray):\n","            true_dist = torch.tensor(true_dist, device=device, dtype=torch.float32)\n","        mse = calculate_mse(estimated_r.cpu(), true_dist.cpu())\n","        all_mse.append(mse)\n","        print(f\"Subset {i+1} MSE: {mse:.6e}\")\n","\n","    mean_mse = np.mean(all_mse)\n","    std_mse = np.std(all_mse)\n","    print(f\"\\nMean MSE: {mean_mse:.6e}, Std MSE: {std_mse:.6e}\")\n","    return all_mse, mean_mse, std_mse"],"metadata":{"id":"TY8wvWX1CmJ4","executionInfo":{"status":"ok","timestamp":1742551355191,"user_tz":-60,"elapsed":1,"user":{"displayName":"zhiyuan wu","userId":"11247997516539609334"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ===============================================\n","# Block [7]: Example usage with data loading\n","# Simplified random sampling for test subsets\n","# ===============================================\n","\n","# Below is a sample code that downloads and loads Tiny-ImageNet-200 data,\n","# trains the model, applies temperature scaling, and evaluates on randomly\n","# sampled test subsets. This block should be placed after Blocks [1]-[6].\n","\n","############################\n","# 1) Data loading\n","############################\n","\n","# Define image transforms (for training and testing)\n","train_transforms = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","test_transforms = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","def download_and_extract_zip(url, dest_folder):\n","    \"\"\"\n","    Download and extract a zip file from the given url to the destination folder.\n","    \"\"\"\n","    zip_path = os.path.join(dest_folder, \"tiny-imagenet.zip\")\n","    if not os.path.exists(dest_folder):\n","        os.makedirs(dest_folder)\n","\n","    response = requests.get(url, stream=True)\n","    with open(zip_path, \"wb\") as file:\n","        for chunk in response.iter_content(chunk_size=1024):\n","            file.write(chunk)\n","\n","    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n","        zip_ref.extractall(dest_folder)\n","    os.remove(zip_path)\n","\n","# Download Tiny-ImageNet-200 (pre-processed) from GitHub\n","url = \"https://github.com/tjmoon0104/pytorch-tiny-imagenet/releases/download/tiny-imagenet-dataset/processed-tiny-imagenet-200.zip\"\n","data_folder = \"./tiny-imagenet-200\"\n","\n","download_and_extract_zip(url, data_folder)\n","\n","train_dir = os.path.join(data_folder, \"tiny-imagenet-200/train\")\n","val_dir   = os.path.join(data_folder, \"tiny-imagenet-200/val\")\n","test_dir  = os.path.join(data_folder, \"tiny-imagenet-200/test\")\n","\n","train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n","val_dataset   = datasets.ImageFolder(val_dir,   transform=test_transforms)\n","test_dataset  = datasets.ImageFolder(test_dir,  transform=test_transforms)\n","\n","def sample_test_set(test_dataset, num_samples):\n","    \"\"\"\n","    Randomly sample 'num_samples' images from the test_dataset.\n","    This is a simpler approach that does not enforce any class distribution.\n","    \"\"\"\n","    indices = list(range(len(test_dataset)))\n","    random.shuffle(indices)\n","    selected_indices = indices[:num_samples]\n","    return Subset(test_dataset, selected_indices)\n","\n","# Example: create multiple test subsets of different sizes\n","test_subset_sizes = [1000, 1500, 2000, 2500, 3000]\n","test_subsets = [sample_test_set(test_dataset, size) for size in test_subset_sizes]\n","\n","# Build DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,  num_workers=4)\n","val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=False, num_workers=4)\n","test_loaders = [DataLoader(subset, batch_size=64, shuffle=False, num_workers=4)\n","                for subset in test_subsets]\n","\n","print(f\"Train Loader: {len(train_loader)} batches, Samples: {len(train_dataset)}\")\n","print(f\"Validation Loader: {len(val_loader)} batches, Samples: {len(val_dataset)}\")\n","for i, tloader in enumerate(test_loaders):\n","    print(f\"Test Loader {i + 1}: {len(tloader)} batches, Samples: {len(test_subsets[i])}\")\n","\n","############################\n","# 2) (Optional) Helper to calculate class distribution\n","############################\n","def calculate_class_distribution(dataset, num_classes=200):\n","    \"\"\"\n","    Count how many samples belong to each class in the dataset,\n","    then return the normalized distribution (length=num_classes).\n","    \"\"\"\n","    labels = [label for _, label in dataset]\n","    class_counts = Counter(labels)\n","    class_distribution = np.zeros(num_classes)\n","    for label, count in class_counts.items():\n","        class_distribution[label] = count\n","    return class_distribution / class_distribution.sum()\n","\n","# Compute the (approximate) true distribution of each test subset\n","test_distributions = []\n","for i, subset in enumerate(test_subsets):\n","    dist = calculate_class_distribution(subset)\n","    test_distributions.append(dist)\n","    print(f\"Test Subset {i + 1} Class Distribution (first 10 classes): {dist[:10]} ...\")\n","\n","############################\n","# 3) Create the model\n","############################\n","model = create_resnet18_for_200_classes()\n","print(\"\\nTrainable parameters:\")\n","for n, p in model.named_parameters():\n","    if p.requires_grad:\n","        print(\"  \", n)\n","\n","############################\n","# 4) Define loss and optimizer\n","############################\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n","                       lr=0.001, weight_decay=1e-4)\n","\n","############################\n","# 5) Train the model\n","############################\n","# Set 'use_entropy_regularizer=True' to add entropy regularization\n","num_epochs = 0  # Example epochs\n","train_model(model,\n","            train_loader,\n","            val_loader,\n","            criterion,\n","            optimizer,\n","            num_epochs=num_epochs,\n","            use_entropy_regularizer=True,\n","            entropy_weight=0.1)\n","\n","############################\n","# 6) Temperature scaling\n","############################\n","model_with_temp = ModelWithTemperature(model).to(device)\n","model_with_temp.set_temperature(val_loader)\n","\n","############################\n","# 7) Evaluate on test subsets\n","############################\n","mse_list, mean_mse, std_mse = evaluate_on_test_subsets(\n","    model_with_temp,\n","    test_loaders,\n","    test_distributions,\n","    num_classes=200\n",")\n","\n","print(\"\\nFinal results:\")\n","print(f\"Mean MSE across subsets: {mean_mse:.6e}\")\n","print(f\"Std MSE across subsets:  {std_mse:.6e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MpwG20IJTY4","executionInfo":{"status":"ok","timestamp":1742551448165,"user_tz":-60,"elapsed":92974,"user":{"displayName":"zhiyuan wu","userId":"11247997516539609334"}},"outputId":"d9fc67d5-898a-490c-f0d5-3d249f2a18a4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Train Loader: 1563 batches, Samples: 100000\n","Validation Loader: 79 batches, Samples: 5000\n","Test Loader 1: 16 batches, Samples: 1000\n","Test Loader 2: 24 batches, Samples: 1500\n","Test Loader 3: 32 batches, Samples: 2000\n","Test Loader 4: 40 batches, Samples: 2500\n","Test Loader 5: 47 batches, Samples: 3000\n","Test Subset 1 Class Distribution (first 10 classes): [0.004 0.005 0.006 0.004 0.002 0.002 0.002 0.005 0.007 0.007] ...\n","Test Subset 2 Class Distribution (first 10 classes): [0.00666667 0.00466667 0.00333333 0.00466667 0.008      0.00333333\n"," 0.00666667 0.00466667 0.00666667 0.00666667] ...\n","Test Subset 3 Class Distribution (first 10 classes): [0.006  0.0055 0.0045 0.0045 0.0035 0.006  0.0045 0.0055 0.006  0.0065] ...\n","Test Subset 4 Class Distribution (first 10 classes): [0.0064 0.0032 0.0052 0.006  0.0048 0.0052 0.0056 0.0036 0.0048 0.004 ] ...\n","Test Subset 5 Class Distribution (first 10 classes): [0.006      0.00533333 0.00566667 0.00366667 0.006      0.00433333\n"," 0.00466667 0.00533333 0.003      0.003     ] ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Trainable parameters:\n","   layer4.0.conv1.weight\n","   layer4.0.bn1.weight\n","   layer4.0.bn1.bias\n","   layer4.0.conv2.weight\n","   layer4.0.bn2.weight\n","   layer4.0.bn2.bias\n","   layer4.0.downsample.0.weight\n","   layer4.0.downsample.1.weight\n","   layer4.0.downsample.1.bias\n","   layer4.1.conv1.weight\n","   layer4.1.bn1.weight\n","   layer4.1.bn1.bias\n","   layer4.1.conv2.weight\n","   layer4.1.bn2.weight\n","   layer4.1.bn2.bias\n","   fc.weight\n","   fc.bias\n","Optimal Temperature: 1.187\n","\n","Evaluating on Test Subset 1...\n","Subset 1 MSE: 1.504253e-02\n","\n","Evaluating on Test Subset 2...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/cvxpy/problems/problem.py:1504: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Subset 2 MSE: 1.494659e-02\n","\n","Evaluating on Test Subset 3...\n","Subset 3 MSE: 1.612223e-02\n","\n","Evaluating on Test Subset 4...\n","Subset 4 MSE: 1.609594e-02\n","\n","Evaluating on Test Subset 5...\n","Subset 5 MSE: 1.657827e-02\n","\n","Mean MSE: 1.575711e-02, Std MSE: 6.465273e-04\n","\n","Final results:\n","Mean MSE across subsets: 1.575711e-02\n","Std MSE across subsets:  6.465273e-04\n"]}]}]}